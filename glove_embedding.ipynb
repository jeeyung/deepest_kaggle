{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_submission.csv\n",
      "test.csv\n",
      "train.csv\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:07, 50278.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000 word vectors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from tqdm import tqdm\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"data\"]).decode(\"utf8\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "max_features = 20000\n",
    "maxlen = 100\n",
    "\n",
    "\n",
    "train = pd.read_csv(\"data/train.csv\",encoding='latin-1',error_bad_lines=False)\n",
    "test = pd.read_csv(\"data/test.csv\")\n",
    "train = train.sample(frac=1)\n",
    "\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"CVxTz\").values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"CVxTz\").values\n",
    "\n",
    "embeddings_index ={}\n",
    "f = open('glove.6B.50d.txt')\n",
    "for line in tqdm(f):\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:],dtype ='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close\n",
    "print('%s word vectors'%len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "from keras.preprocessing import sequence, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 397657/397657 [00:00<00:00, 671483.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143584/143613 [============================>.] - ETA: 0s - loss: 0.3076 - acc: 0.9536Epoch 00001: val_loss improved from inf to 0.31714, saving model to weights_base.best.hdf5\n",
      "143613/143613 [==============================] - 4090s 28ms/step - loss: 0.3075 - acc: 0.9536 - val_loss: 0.3171 - val_acc: 0.9934\n",
      "Epoch 2/2\n",
      "143584/143613 [============================>.] - ETA: 0s - loss: 0.3025 - acc: 0.9913Epoch 00002: val_loss improved from 0.31714 to 0.31630, saving model to weights_base.best.hdf5\n",
      "143613/143613 [==============================] - 4345s 30ms/step - loss: 0.3025 - acc: 0.9913 - val_loss: 0.3163 - val_acc: 0.9934\n"
     ]
    }
   ],
   "source": [
    "token = text.Tokenizer(num_words=None)\n",
    "\n",
    "token.fit_on_texts(list(list_sentences_train) + list(list_sentences_test))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_t = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_te = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "word_index = token.word_index\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 50))\n",
    "for word, i in tqdm(word_index.items()):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "\n",
    "def get_model():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(word_index) + 1,\n",
    "                     50,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=maxlen,\n",
    "                     trainable=False))\n",
    "    model.add(SpatialDropout1D(0.3))\n",
    "    model.add(LSTM(300, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(0.8))\n",
    "\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(0.8))\n",
    "\n",
    "    model.add(Dense(6))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "model = get_model()\n",
    "batch_size = 32\n",
    "epochs = 2\n",
    "\n",
    "\n",
    "file_path=\"weights_base.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=20)\n",
    "\n",
    "\n",
    "callbacks_list = [checkpoint, early] #early\n",
    "model.fit(X_t, y, batch_size=batch_size, epochs=epochs, validation_split=0.1, callbacks=callbacks_list)\n",
    "\n",
    "model.load_weights(file_path)\n",
    "\n",
    "y_test = model.predict(X_te)\n",
    "\n",
    "\n",
    "\n",
    "sample_submission = pd.read_csv(\"data/sample_submission.csv\")\n",
    "\n",
    "sample_submission[list_classes] = y_test\n",
    "\n",
    "\n",
    "\n",
    "sample_submission.to_csv(\"baseline_glove.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
